# RDMA Cluster Network Configuration for vLLM Multi-Node Deployment
# Use with BM.GPU.H100.8 or BM.GPU.A100-v2.8 shapes

servingEngineSpec:
  runtimeClassName: ""
  modelSpec:
  - name: "llama405b"
    repository: "vllm/vllm-openai"
    tag: "latest"
    modelURL: "meta-llama/Llama-3.1-405B-Instruct"

    replicaCount: 1

    # Multi-node parallelism configuration
    tensorParallelSize: 8   # GPUs per node
    pipelineParallelSize: 2 # Number of nodes

    requestCPU: 64
    requestMemory: "512Gi"
    requestGPU: 8

    hf_token: "YOUR_HUGGINGFACE_TOKEN"

    pvcStorage: "1000Gi"
    pvcAccessMode:
      - ReadWriteOnce
    storageClass: "oci-block-storage-hp"

    # NCCL and RDMA environment variables
    env:
      # RDMA/InfiniBand settings
      - name: NCCL_IB_HCA
        value: "mlx5"
      - name: NCCL_IB_GID_INDEX
        value: "3"
      - name: NCCL_IB_SL
        value: "0"
      - name: NCCL_IB_TC
        value: "41"
      - name: NCCL_IB_QPS_PER_CONNECTION
        value: "4"

      # GPUDirect RDMA settings
      - name: NCCL_NET_GDR_LEVEL
        value: "5"
      - name: NCCL_NET_GDR_READ
        value: "1"

      # Network interface
      - name: NCCL_SOCKET_IFNAME
        value: "eth0"

      # Buffer and channel settings
      - name: NCCL_BUFFSIZE
        value: "2097152"
      - name: NCCL_MIN_NCHANNELS
        value: "4"
      - name: NCCL_MAX_NCHANNELS
        value: "8"

      # Debug (set to WARN in production)
      - name: NCCL_DEBUG
        value: "INFO"

      # Timeout settings
      - name: NCCL_TIMEOUT
        value: "600"

      # vLLM distributed settings
      - name: VLLM_HOST_IP
        valueFrom:
          fieldRef:
            fieldPath: status.podIP

    # Node selector for cluster network nodes
    nodeSelector:
      node.kubernetes.io/instance-type: "BM.GPU.H100.8"

    tolerations:
      - key: "nvidia.com/gpu"
        operator: "Exists"
        effect: "NoSchedule"
      - key: "oci.oraclecloud.com/cluster-network"
        operator: "Exists"
        effect: "NoSchedule"

    # vLLM arguments for distributed inference
    extraArgs:
      - "--max-model-len=8192"
      - "--gpu-memory-utilization=0.95"
      - "--tensor-parallel-size=8"
      - "--pipeline-parallel-size=2"
      - "--distributed-executor-backend=ray"
      - "--trust-remote-code"

    # Resource limits for proper scheduling
    resources:
      limits:
        nvidia.com/gpu: 8
        # Request RDMA resources if available
        # rdma/hca_shared_devices_a: 1
      requests:
        nvidia.com/gpu: 8
        cpu: 64
        memory: "512Gi"
