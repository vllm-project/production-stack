# Unified configuration for disaggregated prefill setup
servingEngineSpec:
  enableEngine: true
  runtimeClassName: "nvidia"
  containerPort: 8000
  modelSpec:
    # Prefill node configuration
    - name: "llama-prefill"
      repository: "lmcache/vllm-openai"
      tag: "nightly-2025-09-04"
      modelURL: "facebook/opt-125m"
      replicaCount: 1
      requestCPU: 8
      requestMemory: "30Gi"
      # requestGPU: 1
      pvcStorage: "50Gi"
      vllmConfig:
        enablePrefixCaching: false
        # maxModelLen: 2048
        extraArgs:
          - "--enforce-eager"
          - "--disable-log-requests"
      lmcacheConfig:
        cudaVisibleDevices: "0"
        enabled: true
        kvRole: "kv_producer"
        localCpu: true
        maxLocalCpuSize: 5
        maxLocalDiskSize: 0
        enableNixl: true
        enableXpyd: true
        nixlRole: "sender"
        nixlProxyHost: "vllm-router-service"
        nixlProxyPort: 7500
        nixlBufferSize: "3774873600"
        nixlBufferDevice: "cuda"
        enablePD: true
        rpcPort: "producer1"
      labels:
        model: "llama-prefill"
      chatTemplate: "chat.jinja2"
      chatTemplateConfigMap: |-
        {% for message in messages %}{{'<|im_start|>' + message['role'] + '\n' + message['content']}}{% if (loop.last and add_generation_prompt) or not loop.last %}{{ '<|im_end|>' + '\n'}}{% endif %}{% endfor %}
        {% if add_generation_prompt and messages[-1]['role'] != 'assistant' %}{{ '<|im_start|>assistant\n' }}{% endif %}
      # hf_token: <hf-token>
    # Decode node configuration
    - name: "llama-decode"
      repository: "lmcache/vllm-openai"
      tag: "nightly-2025-09-04"
      modelURL: "facebook/opt-125m"
      replicaCount: 1
      requestCPU: 8
      requestMemory: "30Gi"
      # requestGPU: 1
      pvcStorage: "50Gi"
      vllmConfig:
        enablePrefixCaching: false
        # maxModelLen: 2048
        extraArgs:
          - "--enforce-eager"
          - "--disable-log-requests"
      lmcacheConfig:
        cudaVisibleDevices: "1"
        enabled: true
        kvRole: "kv_consumer"  # Set decode node as consumer
        localCpu: false
        maxLocalCpuSize: 0
        enableNixl: true
        enableXpyd: true
        nixlRole: "receiver"
        nixlPeerHost: "0.0.0.0"
        nixlPeerInitPort: 7300
        nixlPeerAllocPort: 7400
        nixlBufferSize: "3774873600"
        nixlBufferDevice: "cuda"
        # nixlBackends: ["UCX"]
        enablePD: true
        rpcPort: "consumer1"
        skipLastNTokens: 1
      # hf_token: <hf-token>
      labels:
        model: "llama-decode"
      chatTemplate: "chat.jinja2"
      chatTemplateConfigMap: |-
        {% for message in messages %}{{'<|im_start|>' + message['role'] + '\n' + message['content']}}{% if (loop.last and add_generation_prompt) or not loop.last %}{{ '<|im_end|>' + '\n'}}{% endif %}{% endfor %}
        {% if add_generation_prompt and messages[-1]['role'] != 'assistant' %}{{ '<|im_start|>assistant\n' }}{% endif %}
  containerSecurityContext:
    capabilities:
      add:
        - SYS_PTRACE

routerSpec:
  repository: "xiaokunchen/vllm-router"
  tag: "09-10-v9"
  imagePullPolicy: "IfNotPresent"
  strategy:
    type: Recreate
  enableRouter: true
  routingLogic: "roundrobin"
  extraArgs:
    - "--log-level"
    - "info"
