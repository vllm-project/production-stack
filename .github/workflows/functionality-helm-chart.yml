name: Functionality test for helm chart
run-name: ${{ github.actor }} is testing out helm chart functions üöÄ

concurrency:
  group: ${{ github.ref }}
  cancel-in-progress: true

on:
  push:
    branches:
      - main
    paths:
      - ".github/**"
      - "**/*.py"
      - "pyproject.toml"
      - "helm/**"
  pull_request:
    paths:
      - ".github/**"
      - "**/*.py"
      - "pyproject.toml"
      - "helm/**"

jobs:
  e2e-k8s-functionality-test:
    name: E2E Kubernetes Functionality Test
    runs-on: self-hosted
    timeout-minutes: 20
    steps:
      - name: Check out repository code
        uses: actions/checkout@v4
      - name: Verify K8s cluster is ready
        run: |
          kubectl config use-context minikube
          printf "\n=== Cluster Details ===\n"
          kubectl config view
          kubectl cluster-info
          kubectl get nodes -o wide
          kubectl describe nodes
          kubectl get pods -A -o wide
          kubectl get events --sort-by='.lastTimestamp' -A
          helm version
          kubectl version
          printf "\n=== Final Cluster Health Check ===\n"
          kubectl get cs
          kubectl -n kube-system wait --for=condition=Ready pod --all --timeout=300s

      - name: Clean up Docker system before building image
        run: |
          echo "Cleaning up git-act-router Docker system..."
          # Remove only git-act-router related images
          docker images --filter=reference='git-act-router*' -q | xargs -r docker rmi -f
          # Clean up dangling images
          docker image prune --force
      - name: Build and Load Docker image
        env:
          DOCKER_IMAGE_NAME: localhost:5000/git-act-router
        # Add timeout to prevent hanging builds
        timeout-minutes: 15
        run: |
          # Set error handling and verbose mode
          set -eo pipefail

          echo "üî® Building Docker image $DOCKER_IMAGE_NAME..."
          sudo docker build --build-arg INSTALL_OPTIONAL_DEP=default \
            --no-cache \
            -t "$DOCKER_IMAGE_NAME" \
            -f docker/Dockerfile . || {
              echo "‚ùå Docker build failed!"
              exit 1
            }
          echo "‚úÖ Docker image $DOCKER_IMAGE_NAME built successfully!"

          echo "üîç Verifying Docker image..."
          if ! docker images "$DOCKER_IMAGE_NAME" | grep "$DOCKER_IMAGE_NAME"; then
            echo "‚ùå Docker image $DOCKER_IMAGE_NAME not found after build!"
            exit 1
          fi
          echo "‚úÖ Docker image $DOCKER_IMAGE_NAME found after build!"

          sudo sysctl fs.protected_regular=0
          echo "üîç Verifying image digest..."
          if ! docker inspect --format='{{.Id}}' "$DOCKER_IMAGE_NAME" >/dev/null 2>&1; then
            echo "‚ùå Failed to verify image digest for $DOCKER_IMAGE_NAME"
            exit 1
          fi

          echo "üì§ Loading Docker image into K8s cluster..."
          sudo docker push localhost:5000/git-act-router


      - name: Run Helm tests with matrix strategy
        env:
          DOCKER_BUILDKIT: 1
        run: |
          set -e
          TEST_SCENARIOS=(
            ".github/router.yaml routerSpec"
          )

          for SCENARIO in "${TEST_SCENARIOS[@]}"; do
            VALUES_FILE="$(echo "$SCENARIO" | cut -d' ' -f1)"
            echo "üöÄ Deploying scenario with values file: ${VALUES_FILE}"
            helm uninstall vllm || true

            # Verify Helm chart validity with more thorough checks
            echo "üîç Performing Helm lint and template validation..."
            helm lint ./helm || {
              echo "‚ùå Helm lint failed"
              exit 1
            }

            # Dry run for additional validation
            helm install vllm ./helm -f "$VALUES_FILE" --dry-run --debug || {
              echo "‚ùå Helm dry-run failed"
              exit 1
            }

            # Actual installation
            helm install vllm ./helm -f "$VALUES_FILE" --debug || {
              echo "‚ùå Helm install failed"
              kubectl get pods -A
              kubectl describe pod -l environment=router,release=router --namespace=default
              kubectl get events --sort-by='.lastTimestamp'
              exit 1
            }

            echo "‚è≥ Waiting for pods to be ready..."
            RETRIES=5
            for i in $(seq 1 "${RETRIES}"); do
              if kubectl wait --for=condition=ready pod -l environment=router,release=router --namespace=default --timeout=60s; then
                echo "‚úÖ Pods are ready after ${i} attempt(s)"
                kubectl describe pods -n default -l environment=router,release=router
                break
              else
                echo "üö® Attempt ${i}/${RETRIES}: Pods failed to become ready"
                kubectl describe pod -l environment=router,release=router --namespace=default
                kubectl get events --sort-by='.lastTimestamp'
                kubectl logs -l environment=router,release=router --namespace=default --all-containers=true
                if [ "${i}" -eq "${RETRIES}" ]; then
                  echo "‚ùå Pods did not become ready after ${RETRIES} attempts."
                  kubectl logs -l environment=router,release=router --namespace=default --all-containers=true
                  exit 1
                fi
                echo "üîÑ Retrying in 10 seconds..."
                sleep 10
              fi
            done
          done

      - name: Validate the installation of vllm backend pod
        run: |
          set -ex

          # Maximum number of retry attempts
          MAX_RETRIES=10
          # Interval between retries (in seconds)
          INTERVAL=20

          # Loop for MAX_RETRIES attempts
          for ((i=1; i<=MAX_RETRIES; i++)); do
            # Get all pods with 'vllm' in their name
            pods=$(kubectl get pods --no-headers | grep "vllm" | awk '{print $1}')
            if [ -z "$pods" ]; then
              echo "::warning:: No vllm pods found yet, checking again in $INTERVAL seconds..."
              sleep $INTERVAL
              continue
            fi

            total_pods=0
            ready_pods=0

            # Iterate through each vllm pod
            for pod_name in $pods; do
              line=$(kubectl get pod "$pod_name" --no-headers)
              if [ $? -ne 0 ]; then
                echo "::error:: Failed to get pod $pod_name"
                continue
              fi

              # Extract readiness status and phase
              ready=$(echo "$line" | awk '{print $2}')
              status=$(echo "$line" | awk '{print $3}')
              phase=$(kubectl get pod "$pod_name" -o jsonpath='{.status.phase}')

              ((total_pods++))
              timestamp=$(date '+%Y-%m-%d %H:%M:%S')
              echo "::debug::[$timestamp] Checking pod: $pod_name (Status: $status, Ready: $ready, Phase: $phase)"

              # Check if pod is fully ready
              if [[ "$status" == "Running" && "$ready" =~ ^([1-9][0-9]*)/\1$ ]]; then
                ((ready_pods++))
              else
                echo "::error:: Pod $pod_name not ready (Status: $status, Ready: $ready, Phase: $phase)"
                kubectl describe pod "$pod_name"
                kubectl logs "$pod_name" --all-containers --tail=50
              fi
            done

            # Check if all pods are ready
            if (( ready_pods == total_pods )) && (( total_pods > 0 )); then
              echo "::success:: All $total_pods vllm pods are now Ready and in Running state."
              exit 0
            fi

            # Log progress
            echo "::debug:: Not all pods are ready yet. Attempt $i/$MAX_RETRIES. Checking again in $INTERVAL seconds..."
            sleep $INTERVAL
          done

          # Final timeout message
          echo "::error:: Timeout waiting for pods to be ready after $MAX_RETRIES attempts."
          kubectl get pods --all-namespaces
          exit 1
      - name: Curl the router to verify the functionality
        run: |
          set -ex
          kubectl patch service vllm-router-service -p '{"spec":{"type":"NodePort"}}'
          ip=$(minikube ip)
          port=$(kubectl get svc vllm-router-service -o=jsonpath='{.spec.ports[0].nodePort}')

          # Define endpoint and headers
          ENDPOINT="/v1/models"
          HEADERS=("Content-Type: application/json")
          RETRY_COUNT=5
          RETRY_DELAY=10

          echo "üîç Testing API endpoint: http://$ip:$port$ENDPOINT"

          for i in $(seq 1 $RETRY_COUNT); do
            echo "üì° Attempt $i/$RETRY_COUNT..."
            if curl_output=$(curl -v -m 30 ${HEADERS[@]/#/-H } -X GET http://"$ip":"$port""$ENDPOINT" | jq .); then
              # Validate response format
              if echo "$curl_output" | jq -e 'has("data")' > /dev/null || echo "$curl_output" | jq -e 'length > 0' > /dev/null; then
                echo "‚úÖ Received valid response!"
                break
              else
                echo "‚ö†Ô∏è Response format validation failed"
              fi
            else
              echo "‚ö†Ô∏è Request failed"
            fi

            if [ "$i" -eq "$RETRY_COUNT" ]; then
              echo "‚ùå All $RETRY_COUNT attempts failed!"
              kubectl get pods -o wide
              kubectl describe endpoints vllm-router-service
              kubectl logs -l app.kubernetes.io/instance=vllm --tail=50
              exit 1
            fi

            echo "üîÑ Retrying in $RETRY_DELAY seconds..."
            sleep $RETRY_DELAY
          done

          echo "‚úÖ Successfully requested: $curl_output"
          echo "API endpoint is working properly!"
      - name: Clean up resources
        if: always()
        run: |
          set -e
          echo "Cleaning up resources for production stack..."

          # Perform cleanup with retries
          for i in {1..3}; do
            if helm uninstall vllm; then
              echo "‚úÖ Successfully uninstalled Helm chart"
              break
            else
              echo "‚ö†Ô∏è Helm uninstall attempt $i failed, retrying..."
              sleep 5
            fi

            if [ "$i" -eq 3 ]; then
              echo "‚ö†Ô∏è Failed to cleanly uninstall Helm chart"
            fi
          done
