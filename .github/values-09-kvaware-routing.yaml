servingEngineSpec:
  strategy:
    type: Recreate
  runtimeClassName: ""
  modelSpec:
    # Prefill node configuration
    - name: "opt125m-1"
      repository: "lmcache/vllm-openai"
      tag: "v0.3.5"
      modelURL: "facebook/opt-125m"
      replicaCount: 1
      requestCPU: 6
      requestMemory: "30Gi"
      requestGPU: 1
      pvcStorage: "50Gi"
      vllmConfig:
        enablePrefixCaching: true
        maxModelLen: 1024
        gpuMemoryUtilization: 0.8
      lmcacheConfig:
        enabled: true
        cpuOffloadingBufferSize: "10"
        enableController: true
        controllerPort: 9000
        workerPorts: "8001"
        p2pHost: "localhost"
        p2pInitPorts: "30081"
      env:
        - name: LMCACHE_LOG_LEVEL
          value: "DEBUG"
      chatTemplate: "chat.jinja2"
      chatTemplateConfigMap: |-
        {% for message in messages %}{{'<|im_start|>' + message['role'] + '\n' + message['content']}}{% if (loop.last and add_generation_prompt) or not loop.last %}{{ '<|im_end|>' + '\n'}}{% endif %}{% endfor %}
        {% if add_generation_prompt and messages[-1]['role'] != 'assistant' %}{{ '<|im_start|>assistant\n' }}{% endif %}
    # Decode node configuration
    - name: "opt125m-2"
      repository: "lmcache/vllm-openai"
      tag: "v0.3.5"
      modelURL: "facebook/opt-125m"
      replicaCount: 1
      requestCPU: 6
      requestMemory: "30Gi"
      requestGPU: 1
      pvcStorage: "50Gi"
      vllmConfig:
        enablePrefixCaching: true
        maxModelLen: 1024
        v1: 1
        gpuMemoryUtilization: 0.6
      lmcacheConfig:
        enabled: true
        cpuOffloadingBufferSize: "10"
        enableController: true
        controllerPort: 9000
        workerPorts: "8002"
        p2pHost: "localhost"
        p2pInitPorts: "30082"
      env:
        - name: LMCACHE_LOG_LEVEL
          value: "DEBUG"
      chatTemplate: "chat.jinja2"
      chatTemplateConfigMap: |-
        {% for message in messages %}{{'<|im_start|>' + message['role'] + '\n' + message['content']}}{% if (loop.last and add_generation_prompt) or not loop.last %}{{ '<|im_end|>' + '\n'}}{% endif %}{% endfor %}
        {% if add_generation_prompt and messages[-1]['role'] != 'assistant' %}{{ '<|im_start|>assistant\n' }}{% endif %}
  containerSecurityContext:
    capabilities:
      add:
        - SYS_PTRACE

routerSpec:
  repository: "git-act-router"
  imagePullPolicy: "IfNotPresent"
  strategy:
    type: Recreate
  resources:
    requests:
      cpu: "1"
      memory: "2G"
    limits:
      cpu: "1"
      memory: "2G"
  enableRouter: true
  routingLogic: "kvaware"
  extraArgs:
    - "--log-level"
    - "info"
  lmcacheControllerPort: 9000
