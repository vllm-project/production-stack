servingEngineSpec:
  runtimeClassName: ""

  # If you want to use vllm api key, uncomment the following section, you can either use secret or directly set the value
  # Option 1: Secret reference
  # vllmApiKey:
  #   secretName: "vllm-api-key"
  #   secretKey: "VLLM_API_KEY"

  # Option 2: Direct value
  # vllmApiKey:
  #   value: "abc123"

  modelSpec:
    - name: "llama3-8b-instr"
      repository: "vllm/vllm-openai"
      tag: "latest"
      modelURL: "meta-llama/Llama-3.1-8B-Instruct"
      enableLoRA: true

      # Option 1: Direct token
      # hf_token: "your_huggingface_token_here"

      # OR Option 2: Secret reference
      hf_token:
        secretName: "huggingface-credentials"
        secretKey: "HUGGING_FACE_HUB_TOKEN"

      # Other vLLM configs if needed
      vllmConfig:
        maxModelLen: 4096
        dtype: "bfloat16"

      # Mount Hugging Face credentials and configure LoRA settings
      env:
        - name: HUGGING_FACE_HUB_TOKEN
          valueFrom:
            secretKeyRef:
              name: huggingface-credentials
              key: HUGGING_FACE_HUB_TOKEN
        - name: VLLM_ALLOW_RUNTIME_LORA_UPDATING
          value: "True"

      replicaCount: 1

      # Resource requirements for Llama-3.1-8b
      requestCPU: 8
      requestMemory: "32Gi"
      requestGPU: 1

      pvcStorage: "10Gi"
      pvcAccessMode:
        - ReadWriteOnce

  # Add longer startup probe settings
  startupProbe:
    initialDelaySeconds: 60
    periodSeconds: 30
    failureThreshold: 120 # Allow up to 1 hour for startup

routerSpec:
  repository: "lmcache/lmstack-router"
  tag: "lora"
  imagePullPolicy: "IfNotPresent"
  enableRouter: true
