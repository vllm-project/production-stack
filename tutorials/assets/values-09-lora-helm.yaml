# Example values file for LoRA adapter deployment
# This file shows how to configure LoRA adapters in the production-stack Helm chart
servingEngineSpec:
  runtimeClassName: ""
  strategy:
    type: Recreate
  # vllmApiKey is the API key for the vLLM server. It is used to authenticate the vLLM server.
  # vllmApiKey: <your-vllm-api-key>
  modelSpec:
    - name: "llama3"
      repository: "vllm/vllm-openai"
      tag: "latest"
      modelURL: "meta-llama/Llama-3.1-8B-Instruct"
      enableLoRA: true

      # Option 1: Direct token
      hf_token: <your-hf-token>

      # OR Option 2: Secret reference
      # hf_token:
      #   secretName: "huggingface-credentials"
      #   secretKey: "HUGGING_FACE_HUB_TOKEN"

      # Other vLLM configs if needed
      vllmConfig:
        enablePrefixCaching: true
        maxModelLen: 4096
        dtype: "bfloat16"
        v1: 1
        extraArgs: ["--disable-log-requests", "--gpu-memory-utilization", "0.8"]

      # Mount Hugging Face credentials and configure LoRA settings
      env:
        - name: VLLM_ALLOW_RUNTIME_LORA_UPDATING
          value: "True"

      replicaCount: 2

      # Resource requirements for Llama-3.1-8b
      requestCPU: 8
      requestMemory: "32Gi"
      requestGPU: 1

      pvcStorage: "10Gi"
      pvcAccessMode:
        - ReadWriteOnce
  sidecar:
    image: "lmcache/lmstack-sidecar:latest"
    imagePullPolicy: "Always"

# Enable the lora controller (required for LoRA adapters)
loraController:
  enableLoraController: true
  image:
    repository: "lmcache/lmstack-lora-controller"
    tag: "latest"
    pullPolicy: "Always"

# Enable LoRA adapter functionality
loraAdapters:
    # Example 1: Local LoRA adapter
    # - name: "llama3-nemoguard-adapter"
    #   baseModel: "llama3"
    #   # Optional: vLLM API key configuration
    #   # vllmApiKey:
    #   #   secretName: "vllm-api-key"
    #   #   secretKey: "VLLM_API_KEY"
    #   adapterSource:
    #     type: "local"
    #     adapterName: "llama-3.1-nemoguard-8b-topic-control"
    #     adapterPath: "/data/shared-pvc-storage/lora-adapters/llama-3.1-nemoguard-8b-topic-control"
    #   loraAdapterDeploymentConfig:
    #     algorithm: "default"
    #     replicas: 1

    # Example 2: HuggingFace LoRA adapter
    - name: "llama3-nemoguard-adapter"
      baseModel: "llama3"
      adapterSource:
        type: "huggingface"
        adapterName: "llama-3.1-nemoguard-8b-topic-control"
        repository: "nvidia/llama-3.1-nemoguard-8b-topic-control"
        # Optional: Credentials for repositories
        # Option 1: Direct token
        credentials: <your-hf-token>
        # Option 2: Secret reference
        # credentials:
        #   secretName: "hf-token-secret"
        #   secretKey: "HUGGING_FACE_HUB_TOKEN"

      loraAdapterDeploymentConfig:
        algorithm: "default"
        replicas: 1
