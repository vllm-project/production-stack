servingEngineSpec:
  runtimeClassName: ""
  modelSpec:
  - name: "llama3"
    repository: "lmcache/vllm-openai"
    tag: "2025-04-18"
    modelURL: "meta-llama/Llama-3.1-8B-Instruct"
    replicaCount: 1

    requestCPU: 6
    requestMemory: "16Gi"
    requestGPU: 1

    pvcStorage: "50Gi"
    pvcAccessMode:
      - ReadWriteOnce

    vllmConfig:
      enableChunkedPrefill: false
      enablePrefixCaching: false
      maxModelLen: 4096
      dtype: "bfloat16"
      v1: 1
      extraArgs: ["--disable-log-requests", "--gpu-memory-utilization", "0.8"]

    lmcacheConfig:
      enabled: true
      cpuOffloadingBufferSize: "20"
    hf_token: <your-hf-token>

cacheserverSpec:
  # -- Number of replicas
  replicaCount: 1

  # -- Container port
  containerPort: 8080

  # -- Service port
  servicePort: 81

  # -- Serializer/Deserializer type
  serde: "naive"

  # -- Cache server image (reusing the vllm image)
  repository: "lmcache/vllm-openai"
  tag: "2025-04-18"

  # TODO (Jiayi): please adjust this once we have evictor
  # -- router resource requests and limits
  resources:
    requests:
      cpu: "2"
      memory: "8G"
    limits:
      cpu: "2"
      memory: "10G"

  # -- Customized labels for the cache server deployment
  labels:
    environment: "cacheserver"
    release: "cacheserver"
