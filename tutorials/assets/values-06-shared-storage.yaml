servingEngineSpec:
  runtimeClassName: ""
  modelSpec:
  - name: "mistral"
    repository: "lmcache/vllm-openai"
    tag: "latest"
    modelURL: "mistralai/Mistral-7B-Instruct-v0.2"
    replicaCount: 2
    requestCPU: 10
    requestMemory: "40Gi"
    requestGPU: 1
    pvcStorage: "50Gi"
    vllmConfig:
      enablePrefixCaching: true
      maxModelLen: 16384
      v1: 1

    lmcacheConfig:
      enabled: true
      cpuOffloadingBufferSize: "20"
    env:
      - name: LMCACHE_LOG_LEVEL
        value: "DEBUG"

    hf_token: <YOUR HF TOKEN>

    initContainer:
      name: "wait-for-cache-server"
      image: lmcache/vllm-openai:latest-nightly
      command: ["/bin/sh", "-c"]
      args:
        - |
          timeout 60 bash -c '
          while true; do
            /opt/venv/bin/python3 /workspace/LMCache/examples/kubernetes/health_probe.py $(RELEASE_NAME)-cache-server-service $(LMCACHE_SERVER_SERVICE_PORT) && exit 0
            echo "Waiting for LMCache server..."
            sleep 2
          done'




cacheserverSpec:
  # -- Number of replicas
  replicaCount: 1

  # -- Container port
  containerPort: 8080

  # -- Service port
  servicePort: 81

  # -- Serializer/Deserializer type
  serde: "naive"

  # -- Cache server image (reusing the vllm image)
  repository: "lmcache/vllm-openai"
  tag: "latest-nightly"

  # TODO (Jiayi): please adjust this once we have evictor
  # -- router resource requests and limits
  resources:
    requests:
      cpu: "4"
      memory: "8G"
    limits:
      cpu: "4"
      memory: "10G"

  # -- (Optional) Liveness probe for the cache server, available in vllm-openai:latest-nightly
  # livenessProbe:
  #   initialDelaySeconds: 30
  #   periodSeconds: 10
  #   failureThreshold: 3
  #   timeoutSeconds: 5

  # -- Customized labels for the cache server deployment
  labels:
    environment: "cacheserver"
    release: "cacheserver"

routerSpec:
  resources:
    requests:
      cpu: "1"
      memory: "2G"
    limits:
      cpu: "1"
      memory: "2G"
  routingLogic: "session"
  sessionKey: "x-user-id"
