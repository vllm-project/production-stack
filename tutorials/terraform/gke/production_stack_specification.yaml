servingEngineSpec:
  strategy:
    type: Recreate
  runtimeClassName: ""
  nodeSelector:
    app: vllm-inference
    nvidia.com/gpu: present
  tolerations:
  - key: nvidia.com/gpu
    operator: Equal
    value: present
    effect: NoSchedule
  modelSpec:
  - name: "opt125m"
    repository: "vllm/vllm-openai"
    tag: "latest"
    modelURL: "facebook/opt-125m"

    replicaCount: 1

    requestCPU: 1
    requestMemory: "8Gi"
    requestGPU: 1

    pvcStorage: "10Gi"
    pvcAccessMode:
      - ReadWriteOnce
  serviceMonitor:
    enabled: true

routerSpec:
  nodeSelector:
    app: mgmt-node
  resources:
    requests:
      cpu: 1
      memory: 4G
    limits:
      cpu: 2
      memory: 8G
  serviceMonitor:
    enabled: true

grafanaDashboards:
  enabled: true

kube-prometheus-stack:
  enabled: true

prometheus-adapter:
  enabled: true
  rules:
    default: true
    custom:
    # Example metric to export for HPA
    - seriesQuery: '{__name__=~"^vllm:num_requests_waiting$"}'
      resources:
        overrides:
          namespace:
            resource: "namespace"
      name:
        matches: ""
        as: "vllm_num_requests_waiting"
      metricsQuery: sum by(namespace) (vllm:num_requests_waiting)

    # Export num_incoming_requests_total by model name
    - seriesQuery: '{__name__=~"^vllm:num_incoming_requests_total$"}'
      resources:
        overrides:
          namespace:
            resource: "namespace"
      name:
        matches: ""
        as: "vllm_num_incoming_requests_total"
      metricsQuery: sum by(namespace, model) (vllm:num_incoming_requests_total)
