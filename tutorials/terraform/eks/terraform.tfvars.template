# terraform.tfvars.template
# This template file shows all available Terraform variables for the vLLM EKS deployment.
# Copy this file to terraform.tfvars and fill in your specific values.
# Remember to fill in sensitive values (like AWS keys, HF token)
# and adjust defaults as needed for your environment.

################################################################################
# AWS Credentials and Region
################################################################################
aws_access_key = ""         # Fill your AWS Access Key ID
aws_secret_key = ""         # Fill your AWS Secret Access Key
aws_profile    = ""         # Fill your AWS Profile name (e.g., default, cloudthrill)
region         = "us-east-2" # AWS Region for your deployment

################################################################################
# EKS Cluster Configuration
################################################################################
# ‚ò∏Ô∏è EKS cluster basics
cluster_name    = "vllm-eks-prod" # EKS cluster name
cluster_version = "1.30"          # Kubernetes cluster version

################################################################################
# ü§ñ NVIDIA setup selector
#   ‚Ä¢ plugin           -> device-plugin only
#   ‚Ä¢ operator_no_driver -> GPU Operator (driver disabled)
#   ‚Ä¢ operator_custom  -> GPU Operator with your YAML
################################################################################
nvidia_setup = "plugin"

################################################################################
# üß† LLM Inference Configuration
################################################################################
enable_vllm           = true  # Set to true to deploy vLLM
hf_token              = ""    # Hugging Face token for model download (if needed)
inference_hardware    = "gpu" # "cpu" or "gpu"

# Paths to Helm chart values templates for vLLM.
# These paths are relative to the root of your Terraform project.
gpu_vllm_helm_config = "./modules/llm-stack/helm/gpu/gpu-tinyllama-light-ingress.tpl"
cpu_vllm_helm_config = "./modules/llm-stack/helm/cpu/cpu-tinyllama-light-ingress.tpl"

################################################################################
# ‚öôÔ∏è Node-group sizing
################################################################################
# CPU pool (always present)
cpu_node_min_size     = 1
cpu_node_max_size     = 3
cpu_node_desired_size = 2

# GPU pool (ignored unless inference_hardware = "gpu")
gpu_node_min_size     = 1
gpu_node_max_size     = 1
gpu_node_desired_size = 1
# gpu_node_instance_types = ["g6.2xlarge"]
# gpu_capacity_type = "SPOT"
################################################################################
# üíæ Storage CSI drivers (Uncomment and set to true to enable)
################################################################################
# enable_efs_csi_driver = true  # Enable EFS CSI driver
# enable_efs_storage    = false # Enable EFS storage
# enable_iam_roles      = false # Enable EFS CSI driver IAM roles

################################################################################
# üìà Observability (Uncomment and set to true to enable)
################################################################################
# enable_cert_manager = true # Enable cert-manager
# enable_prometheus   = true # Enable Prometheus
# enable_grafana      = true # Enable Grafana

################################################################################
# üåê VPC Configuration
################################################################################
vpc_cidr = "10.0.0.0/16" # VPC CIDR block
enable_lb_ctl = false # Enable AWS Load Balancer Controller default true
# For complex list variables, define them as HCL lists:
# public_subnet_cidrs  = ["10.0.1.0/24", "10.0.2.0/24", "10.0.3.0/24"]
# private_subnet_cidrs = ["10.0.4.0/24", "10.0.5.0/24", "10.0.6.0/24"]
# vpc_name = "vllm-vpc-prod"
