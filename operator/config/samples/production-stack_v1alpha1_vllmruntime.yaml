apiVersion: production-stack.vllm.ai/v1alpha1
kind: VLLMRuntime
metadata:
  labels:
    app.kubernetes.io/name: production-stack
    app.kubernetes.io/managed-by: kustomize
    model: Llama-3.1-8B-Instruct
  name: vllmruntime-sample
spec:
  # Model configuration
  model:
    modelURL: "meta-llama/Llama-3.1-8B-Instruct"
    enableLoRA: true
    enableTool: false
    toolCallParser: ""
    maxModelLen: 4096
    dtype: "bfloat16"
    maxNumSeqs: 32
    # HuggingFace token secret (optional)
    hfTokenSecret:
      name: "huggingface-token"
    hfTokenName: "token"

  # vLLM server configuration
  vllmConfig:
    # vLLM specific configurations
    enableChunkedPrefill: false
    enablePrefixCaching: false
    tensorParallelSize: 1
    gpuMemoryUtilization: "0.8"
    maxLoras: 4
    extraArgs: ["--disable-log-requests"]
    v1: true
    port: 8000
    # Environment variables
    env:
      - name: HF_HOME
        value: "/data"

  # LM Cache configuration
  lmCacheConfig:
    enabled: true
    cpuOffloadingBufferSize: "15"
    diskOffloadingBufferSize: "0"
    remoteUrl: "lm://cacheserver-sample.default.svc.cluster.local:80"
    remoteSerde: "naive"

  # Deployment configuration
  deploymentConfig:
    # Resource requirements
    resources:
      cpu: "5"
      memory: "32Gi"
      gpu: "1"

    # Image configuration
    image:
      registry: "docker.io"
      name: "lmcache/vllm-openai:2025-05-27-v1"
      pullPolicy: "IfNotPresent"
      pullSecretName: ""

    # Number of replicas
    replicas: 2

    # Deployment strategy
    deploymentStrategy: "Recreate"

    # Sidecar configuration
    sidecarConfig:
      enabled: true
      name: "sidecar"
      image:
        registry: "docker.io"
        name: "lmcache/lmstack-sidecar:latest"
        pullPolicy: "Always"
      resources:
        cpu: "0.5"
        memory: "128Mi"
      mountPath: "/data"

  # Storage configuration
  storageConfig:
    enabled: true
    storageClassName: ""
    size: "10Gi"
    accessMode: "ReadWriteMany"
    mountPath: "/data"
