apiVersion: production-stack.vllm.ai/v1alpha1
kind: VLLMRuntime
metadata:
  labels:
    app.kubernetes.io/name: production-stack
    app.kubernetes.io/managed-by: kustomize
    model: Llama-3.2-1B-Instruct
  name: vllmruntime-pd-sample
spec:
  enablePDDisaggregation: true

  topology:
    # Prefill node configuration
    prefill:
      model:
        modelURL: "meta-llama/Llama-3.2-1B-Instruct"
        enableLoRA: true
        enableTool: false
        toolCallParser: ""
        maxModelLen: 4096
        dtype: "bfloat16"
        maxNumSeqs: 32
        hfTokenSecret:
          name: "huggingface-token"
        hfTokenName: "token"

      vllmConfig:
        enableChunkedPrefill: false
        enablePrefixCaching: false
        tensorParallelSize: 1
        gpuMemoryUtilization: "0.8"
        maxLoras: 4
        extraArgs: ["--disable-log-requests"]
        v1: true
        port: 8000
        env:
          - name: HF_HOME
            value: "/data"
          - name: TRANSFORMERS_CACHE
            value: "/data/transformers_cache"
          - name: HF_HUB_CACHE
            value: "/data/hf_hub_cache"
          - name: PYTORCH_CUDA_ALLOC_CONF
            value: "expandable_segments:True"
          - name: VLLM_CPU_KVCACHE_SPACE
            value: "0"
          - name: PYTHONHASHSEED
            value: "0"

      lmCacheConfig:
        enabled: true
        kvRole: "kv_producer"
        localCpu: ""
        maxLocalCpuSize: 0
        enableNixl: true
        enableXpyd: false
        nixlRole: "sender"
        nixlProxyHost: "vllmrouter-sample"
        nixlProxyPort: "7500"
        nixlBufferSize: "58720256"
        nixlBufferDevice: "cuda"
        rpcPort: "producer1"
        cpuOffloadingBufferSize: ""
        diskOffloadingBufferSize: "0"
        remoteUrl: ""
        remoteSerde: "naive"

      storageConfig:
        enabled: true
        storageClassName: "local-path"
        size: "10Gi"
        accessMode: "ReadWriteOnce"
        mountPath: "/data"

      deploymentConfig:
        replicas: 2
        deploymentStrategy: "Recreate"
        resources:
          cpu: "2"
          memory: "16Gi"
          gpu: "1"
        image:
          registry: "docker.io"
          name: "lmcache/vllm-openai:v0.3.7"
          pullPolicy: "IfNotPresent"
          pullSecretName: ""
        sidecarConfig:
          enabled: true
          name: "sidecar"
          image:
            registry: "docker.io"
            name: "lmcache/lmstack-sidecar:latest"
            pullPolicy: "Always"
          resources:
            cpu: "0.5"
            memory: "128Mi"
          mountPath: "/data"

    # Decode node configuration
    decode:
      model:
        modelURL: "meta-llama/Llama-3.2-1B-Instruct"
        enableLoRA: true
        enableTool: false
        toolCallParser: ""
        maxModelLen: 4096
        dtype: "bfloat16"
        maxNumSeqs: 32
        hfTokenSecret:
          name: "huggingface-token"
        hfTokenName: "token"

      vllmConfig:
        enableChunkedPrefill: false
        enablePrefixCaching: false
        tensorParallelSize: 1
        gpuMemoryUtilization: "0.8"
        maxLoras: 4
        extraArgs: ["--disable-log-requests"]
        v1: true
        port: 8000
        env:
          - name: HF_HOME
            value: "/data"
          - name: TRANSFORMERS_CACHE
            value: "/data/transformers_cache"
          - name: HF_HUB_CACHE
            value: "/data/hf_hub_cache"
          - name: PYTORCH_CUDA_ALLOC_CONF
            value: "expandable_segments:True"
          - name: VLLM_CPU_KVCACHE_SPACE
            value: "0"
          - name: PYTHONHASHSEED
            value: "0"

      lmCacheConfig:
        enabled: true
        kvRole: "kv_consumer"
        localCpu: ""
        maxLocalCpuSize: 0
        enableNixl: true
        enableXpyd: false
        nixlRole: "receiver"
        nixlPeerHost: "0.0.0.0"
        nixlPeerInitPort: "7300"
        nixlPeerAllocPort: "7400"
        nixlBufferSize: "58720256"
        nixlBufferDevice: "cuda"
        rpcPort: "consumer1"
        skipLastNTokens: 1
        cpuOffloadingBufferSize: ""
        diskOffloadingBufferSize: "0"
        remoteUrl: ""
        remoteSerde: "naive"

      storageConfig:
        enabled: true
        storageClassName: "local-path"
        size: "10Gi"
        accessMode: "ReadWriteOnce"
        mountPath: "/data"

      deploymentConfig:
        replicas: 2
        deploymentStrategy: "Recreate"
        resources:
          cpu: "2"
          memory: "16Gi"
          gpu: "1"
        image:
          registry: "docker.io"
          name: "lmcache/vllm-openai:v0.3.7"
          pullPolicy: "IfNotPresent"
          pullSecretName: ""
        sidecarConfig:
          enabled: true
          name: "sidecar"
          image:
            registry: "docker.io"
            name: "lmcache/lmstack-sidecar:latest"
            pullPolicy: "Always"
          resources:
            cpu: "0.5"
            memory: "128Mi"
          mountPath: "/data"
